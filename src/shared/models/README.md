# Database Models

This directory contains the SQLAlchemy models for the healthcare image analysis system.

## Models

### AnalysisJob

Tracks MRI image analysis jobs and their processing status.

**Fields:**
- `job_id` (UUID): Unique identifier for the analysis job
- `user_id` (String): Identifier for the user who submitted the job
- `original_image_key` (String): S3 key for the original MRI image
- `status` (JobStatus): Current processing status
- `created_at` (DateTime): Timestamp when the job was created
- `updated_at` (DateTime): Timestamp when the job was last updated
- `error_message` (Text): Error message if the job failed

**Relationships:**
- `results`: One-to-many relationship with AnalysisResult

### AnalysisResult

Stores results from each stage of the analysis pipeline.

**Fields:**
- `result_id` (UUID): Unique identifier for the analysis result
- `job_id` (UUID): Reference to the parent analysis job
- `segmentation_result_key` (String): S3 key for the segmentation result image
- `image_description` (Text): Text description generated by the VLM model
- `enhanced_report` (Text): Enhanced medical report generated by LLM with RAG
- `confidence_scores` (JSON): Confidence scores from each processing stage
- `processing_metrics` (JSON): Processing time and performance metrics
- `created_at` (DateTime): Timestamp when the result was created

**Relationships:**
- `job`: Many-to-one relationship with AnalysisJob

### JobStatus

Enumeration for job processing status.

**Values:**
- `UPLOADED`: Image has been uploaded and is queued for processing
- `SEGMENTING`: Image is being processed by the segmentation model
- `CONVERTING`: Segmented image is being converted to text by VLM
- `ENHANCING`: Text is being enhanced by LLM with RAG
- `COMPLETED`: All processing stages completed successfully
- `FAILED`: Processing failed at some stage

## Usage

```python
from shared.models.database import AnalysisJob, AnalysisResult, JobStatus

# Create a new job
job = AnalysisJob(
    user_id="doctor_123",
    original_image_key="s3://bucket/image.nii",
    status=JobStatus.UPLOADED
)

# Create a result
result = AnalysisResult(
    job_id=job.job_id,
    image_description="Normal brain structure",
    confidence_scores={"segmentation": 0.95}
)

# Convert to dictionary
job_dict = job.to_dict()
result_dict = result.to_dict()
```

## Database Schema

The models create the following PostgreSQL tables:

```sql
-- Job status enum
CREATE TYPE job_status AS ENUM (
    'uploaded', 'segmenting', 'converting', 
    'enhancing', 'completed', 'failed'
);

-- Analysis jobs table
CREATE TABLE analysis_jobs (
    job_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id VARCHAR(255) NOT NULL,
    original_image_key VARCHAR(500) NOT NULL,
    status job_status NOT NULL DEFAULT 'uploaded',
    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    error_message TEXT
);

-- Analysis results table
CREATE TABLE analysis_results (
    result_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    job_id UUID NOT NULL REFERENCES analysis_jobs(job_id) ON DELETE CASCADE,
    segmentation_result_key VARCHAR(500),
    image_description TEXT,
    enhanced_report TEXT,
    confidence_scores JSONB,
    processing_metrics JSONB,
    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW()
);
```

## Indexes

The following indexes are created for optimal query performance:

- `idx_analysis_jobs_user_id`: Index on user_id for user-specific queries
- `idx_analysis_jobs_status`: Index on status for status-based filtering
- `idx_analysis_jobs_created_at`: Index on created_at for time-based queries
- `idx_analysis_results_job_id`: Index on job_id for result lookups
- `idx_analysis_results_created_at`: Index on created_at for time-based queries

## Triggers

- `update_analysis_jobs_updated_at`: Automatically updates the `updated_at` field when a job record is modified