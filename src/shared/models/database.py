"""
Database models for the healthcare image analysis system.

This module contains SQLAlchemy models for tracking analysis jobs and storing results.
"""

from datetime import datetime
from enum import Enum as PyEnum
from typing import Optional, Dict, Any
import uuid

from sqlalchemy import (
    Column, String, Text, DateTime, JSON, ForeignKey, 
    Enum, create_engine, MetaData
)
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship, sessionmaker
from sqlalchemy.sql import func

# Create base class for all models
Base = declarative_base()

# Metadata for migrations
metadata = MetaData()


class JobStatus(PyEnum):
    """Enumeration for job processing status."""
    UPLOADED = "uploaded"
    SEGMENTING = "segmenting"
    CONVERTING = "converting"
    ENHANCING = "enhancing"
    COMPLETED = "completed"
    FAILED = "failed"


class AnalysisJob(Base):
    """
    Model for tracking MRI image analysis jobs.
    
    This table stores information about each analysis job including
    status, timestamps, and error information.
    """
    __tablename__ = "analysis_jobs"
    
    job_id = Column(
        UUID(as_uuid=True), 
        primary_key=True, 
        default=uuid.uuid4,
        comment="Unique identifier for the analysis job"
    )
    user_id = Column(
        String(255), 
        nullable=False,
        comment="Identifier for the user who submitted the job"
    )
    original_image_key = Column(
        String(500), 
        nullable=False,
        comment="S3 key for the original MRI image"
    )
    status = Column(
        Enum(JobStatus), 
        nullable=False, 
        default=JobStatus.UPLOADED,
        comment="Current processing status of the job"
    )
    created_at = Column(
        DateTime(timezone=True), 
        nullable=False, 
        server_default=func.now(),
        comment="Timestamp when the job was created"
    )
    updated_at = Column(
        DateTime(timezone=True), 
        nullable=False, 
        server_default=func.now(),
        onupdate=func.now(),
        comment="Timestamp when the job was last updated"
    )
    error_message = Column(
        Text,
        nullable=True,
        comment="Error message if the job failed"
    )
    
    # Relationship to results
    results = relationship("AnalysisResult", back_populates="job", cascade="all, delete-orphan")
    
    def __repr__(self):
        return f"<AnalysisJob(job_id={self.job_id}, status={self.status.value}, user_id={self.user_id})>"
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert the model instance to a dictionary."""
        return {
            "job_id": str(self.job_id),
            "user_id": self.user_id,
            "original_image_key": self.original_image_key,
            "status": self.status.value,
            "created_at": self.created_at.isoformat() if self.created_at else None,
            "updated_at": self.updated_at.isoformat() if self.updated_at else None,
            "error_message": self.error_message
        }


class AnalysisResult(Base):
    """
    Model for storing analysis results and outputs.
    
    This table stores the results from each stage of the analysis pipeline
    including segmentation results, image descriptions, and enhanced reports.
    """
    __tablename__ = "analysis_results"
    
    result_id = Column(
        UUID(as_uuid=True), 
        primary_key=True, 
        default=uuid.uuid4,
        comment="Unique identifier for the analysis result"
    )
    job_id = Column(
        UUID(as_uuid=True), 
        ForeignKey("analysis_jobs.job_id", ondelete="CASCADE"),
        nullable=False,
        comment="Reference to the parent analysis job"
    )
    segmentation_result_key = Column(
        String(500),
        nullable=True,
        comment="S3 key for the segmentation result image"
    )
    image_description = Column(
        Text,
        nullable=True,
        comment="Text description generated by the VLM model"
    )
    enhanced_report = Column(
        Text,
        nullable=True,
        comment="Enhanced medical report generated by LLM with RAG"
    )
    confidence_scores = Column(
        JSON,
        nullable=True,
        comment="JSON object containing confidence scores from each processing stage"
    )
    processing_metrics = Column(
        JSON,
        nullable=True,
        comment="JSON object containing processing time and performance metrics"
    )
    created_at = Column(
        DateTime(timezone=True), 
        nullable=False, 
        server_default=func.now(),
        comment="Timestamp when the result was created"
    )
    
    # Relationship to job
    job = relationship("AnalysisJob", back_populates="results")
    
    def __repr__(self):
        return f"<AnalysisResult(result_id={self.result_id}, job_id={self.job_id})>"
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert the model instance to a dictionary."""
        return {
            "result_id": str(self.result_id),
            "job_id": str(self.job_id),
            "segmentation_result_key": self.segmentation_result_key,
            "image_description": self.image_description,
            "enhanced_report": self.enhanced_report,
            "confidence_scores": self.confidence_scores,
            "processing_metrics": self.processing_metrics,
            "created_at": self.created_at.isoformat() if self.created_at else None
        }